Metadata-Version: 2.4
Name: efficient-diffusion-loader
Version: 0.1.0
Summary: Optimize Stable Diffusion inference on consumer hardware with Fractional Batching.
Home-page: https://github.com/yourusername/efficient-diffusion-loader
Author: Yash Sinha
Author-email: your.email@example.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: diffusers>=0.20.0
Requires-Dist: transformers
Requires-Dist: accelerate
Requires-Dist: numpy
Requires-Dist: pillow
Requires-Dist: tqdm
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Efficient Diffusion Loader ðŸš€

**Generate 4K images on consumer GPUs (8GB VRAM) using Fractional Batching.**


## âš¡ The Problem
State-of-the-art Diffusion models (like SDXL) require massive VRAM to decode high-resolution images. 
- Decoding a **1024x1024** image is easy.
- Decoding a **4096x4096 (4K)** image requires >24GB VRAM, crashing most consumer cards (RTX 3060/4060).

## ðŸ’¡ The Solution: Fractional VAE Decoding
This library implements **Tiled VAE Decoding** with **Gaussian Blending**. Instead of decoding the entire latent tensor at once, we:
1. Slice the latent into overlapping small tiles (e.g., 64x64).
2. Decode them sequentially on the GPU.
3. Stitch them back together on the CPU using a Gaussian weight mask to ensure **zero visible seams**.

This reduces peak memory usage by **~50%**, allowing you to generate poster-quality images on limited hardware.

## ðŸ“¦ Installation

```bash
pip install efficient-diffusion-loader
